{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.0'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a session which allows spark to run in background and start parallelizing\n",
    "# alternate: spark = SparkSession.builder.appName(\"Name_here\").getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"iris_class\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal.length: double (nullable = true)\n",
      " |-- sepal.width: double (nullable = true)\n",
      " |-- petal.length: double (nullable = true)\n",
      " |-- petal.width: double (nullable = true)\n",
      " |-- variety: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data from csv and specify that there is a header and take the schema as u want\n",
    "\n",
    "df = spark.read.csv('iris.csv', header = True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- variety: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumnRenamed(\"sepal.length\",\"sepal_length\")\\\n",
    "    .withColumnRenamed(\"sepal.width\",\"sepal_width\")\\\n",
    "    .withColumnRenamed(\"petal.length\",\"petal_length\")\\\n",
    "    .withColumnRenamed(\"petal.width\",\"petal_width\")\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also create our own schema,%%!\n",
    "\n",
    "schema = StructType( [\n",
    "                        StructField('sepal.length', DoubleType()),\n",
    "                        StructField('sepal.width', DoubleType()),\n",
    "                        StructField('petal.length', DoubleType()),\n",
    "                        StructField('petal.width', DoubleType()),\n",
    "                        StructField('variety', StringType()),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal.length: double (nullable = true)\n",
      " |-- sepal.width: double (nullable = true)\n",
      " |-- petal.length: double (nullable = true)\n",
      " |-- petal.width: double (nullable = true)\n",
      " |-- variety: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.csv('iris.csv', header = True, schema = schema)\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal.length|sepal.width|petal.length|petal.width|variety|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| Setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| Setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| Setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| Setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| Setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to create another column which is a vector that contains all of our data\n",
    "\n",
    "input_col = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "vectorizer = VectorAssembler(inputCols = input_col, outputCol = 'features')\n",
    "\n",
    "new_df = vectorizer.transform(df2)\n",
    "\n",
    "# Everthing that we do on spark is going to apply transformation to the same table generating new columns in it.\n",
    "\n",
    "# and spark will parallelize this operation based on the number of cores that are available in the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|variety|         features|\n",
      "+------------+-----------+------------+-----------+-------+-----------------+\n",
      "|         5.1|        3.5|         1.4|        0.2| Setosa|[5.1,3.5,1.4,0.2]|\n",
      "|         4.9|        3.0|         1.4|        0.2| Setosa|[4.9,3.0,1.4,0.2]|\n",
      "|         4.7|        3.2|         1.3|        0.2| Setosa|[4.7,3.2,1.3,0.2]|\n",
      "|         4.6|        3.1|         1.5|        0.2| Setosa|[4.6,3.1,1.5,0.2]|\n",
      "|         5.0|        3.6|         1.4|        0.2| Setosa|[5.0,3.6,1.4,0.2]|\n",
      "+------------+-----------+------------+-----------+-------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the target column\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+-----------------+---------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|variety|         features|indexed_variety|\n",
      "+------------+-----------+------------+-----------+-------+-----------------+---------------+\n",
      "|         5.1|        3.5|         1.4|        0.2| Setosa|[5.1,3.5,1.4,0.2]|            0.0|\n",
      "|         4.9|        3.0|         1.4|        0.2| Setosa|[4.9,3.0,1.4,0.2]|            0.0|\n",
      "|         4.7|        3.2|         1.3|        0.2| Setosa|[4.7,3.2,1.3,0.2]|            0.0|\n",
      "|         4.6|        3.1|         1.5|        0.2| Setosa|[4.6,3.1,1.5,0.2]|            0.0|\n",
      "|         5.0|        3.6|         1.4|        0.2| Setosa|[5.0,3.6,1.4,0.2]|            0.0|\n",
      "+------------+-----------+------------+-----------+-------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol='variety',outputCol='indexed_variety')\n",
    "new_df = indexer.fit(new_df).transform(new_df)\n",
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+-----------------+---------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|variety|         features|indexed_variety|\n",
      "+------------+-----------+------------+-----------+-------+-----------------+---------------+\n",
      "|         4.3|        3.0|         1.1|        0.1| Setosa|[4.3,3.0,1.1,0.1]|            0.0|\n",
      "|         4.4|        3.0|         1.3|        0.2| Setosa|[4.4,3.0,1.3,0.2]|            0.0|\n",
      "|         4.4|        3.2|         1.3|        0.2| Setosa|[4.4,3.2,1.3,0.2]|            0.0|\n",
      "|         4.5|        2.3|         1.3|        0.3| Setosa|[4.5,2.3,1.3,0.3]|            0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2| Setosa|[4.6,3.2,1.4,0.2]|            0.0|\n",
      "+------------+-----------+------------+-----------+-------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# randomSplit takes doubles or else throws error, https://stackoverflow.com/questions/68313877/i-get-py4jjavaerror-when-using-randomsplit-on-google-colab\n",
    "df_train, df_test = new_df.randomSplit([80., 20.], seed=13) #we can also include validation\n",
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows and columns in complete dataframe (150, 7)\n",
      "Total rows and columns in complete Train (123, 7)\n",
      "Total rows and columns in complete Test (27, 7)\n"
     ]
    }
   ],
   "source": [
    "print('Total rows and columns in complete dataframe', (new_df.count(), len(new_df.columns)))\n",
    "print('Total rows and columns in complete Train', (df_train.count(), len(df_train.columns)))\n",
    "print('Total rows and columns in complete Test', (df_test.count(), len(df_test.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(featuresCol='features', labelCol='indexed_variety')\n",
    "rf_clf = rf_clf.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+----------+-----------------+---------------+--------------------+--------------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|   variety|         features|indexed_variety|       rawPrediction|         probability|prediction|\n",
      "+------------+-----------+------------+-----------+----------+-----------------+---------------+--------------------+--------------------+----------+\n",
      "|         4.4|        2.9|         1.4|        0.2|    Setosa|[4.4,2.9,1.4,0.2]|            0.0|      [20.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.1|         1.5|        0.2|    Setosa|[4.6,3.1,1.5,0.2]|            0.0|      [20.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|         4.8|        3.0|         1.4|        0.3|    Setosa|[4.8,3.0,1.4,0.3]|            0.0|      [20.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|         5.0|        2.3|         3.3|        1.0|Versicolor|[5.0,2.3,3.3,1.0]|            1.0|[0.0,19.979166666...|[0.0,0.9989583333...|       1.0|\n",
      "|         5.0|        3.0|         1.6|        0.2|    Setosa|[5.0,3.0,1.6,0.2]|            0.0|      [20.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "+------------+-----------+------------+-----------+----------+-----------------+---------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = rf_clf.transform(df_test)\n",
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+---------------+--------------------+--------------------+----------+\n",
      "|   variety|         features|indexed_variety|       rawPrediction|         probability|prediction|\n",
      "+----------+-----------------+---------------+--------------------+--------------------+----------+\n",
      "|    Setosa|[4.4,2.9,1.4,0.2]|            0.0|      [20.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|    Setosa|[4.6,3.1,1.5,0.2]|            0.0|      [20.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|    Setosa|[4.8,3.0,1.4,0.3]|            0.0|      [20.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "|Versicolor|[5.0,2.3,3.3,1.0]|            1.0|[0.0,19.979166666...|[0.0,0.9989583333...|       1.0|\n",
      "|    Setosa|[5.0,3.0,1.6,0.2]|            0.0|      [20.0,0.0,0.0]|       [1.0,0.0,0.0]|       0.0|\n",
      "+----------+-----------------+---------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.select('variety', 'features', 'indexed_variety', 'rawPrediction', 'probability', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = MulticlassClassificationEvaluator(labelCol='indexed_variety', predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = criterion.evaluate(df_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
